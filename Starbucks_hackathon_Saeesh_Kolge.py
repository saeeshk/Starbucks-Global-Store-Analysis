# -*- coding: utf-8 -*-
"""Starbucks_Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1duD-LllHjIPyeYK8sAGMQHx8Fdz73F7k
"""

'''
Starbucks Store Locations
https://www.kaggle.com/datasets/omarsobhy14/starbucks-store-location-2023?select=Starbucks+Store+Locations.xlsx.

'''
import os
import math
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt


try:
    from ace_tools import display_dataframe_to_user
except Exception:
    display_dataframe_to_user = None

xlsx_path = "/content/Starbucks Store Locations.xlsx"
if not os.path.exists(xlsx_path):
    raise FileNotFoundError(f"Expected file not found at {xlsx_path}")

xls = pd.ExcelFile(xlsx_path)
print("Sheets in workbook:", xls.sheet_names)

sheet_frames = {name: pd.read_excel(xls, sheet_name=name) for name in xls.sheet_names}
main_sheet_name = max(sheet_frames.keys(), key=lambda n: len(sheet_frames[n]))
df = sheet_frames[main_sheet_name].copy()
print(f"Using sheet: {main_sheet_name} with {len(df)} rows\n")

#  Standardize columns
orig_cols = df.columns.tolist()
def clean_col(c):
    return (
        str(c).strip()
        .lower()
        .replace(" ", "_")
        .replace("-", "_")
        .replace("/", "_")
    )
df.columns = [clean_col(c) for c in df.columns]
print("Original columns -> cleaned columns")
for o, c in zip(orig_cols, df.columns):
    print(f"  {o} -> {c}")

#  Quick preview
print("\nPreview (first 5 rows):")
display_preview = df.head(5)
print(display_preview.to_string(index=False))

#  Identify candidate text columns to translate/clean
text_cols = df.select_dtypes(include=['object']).columns.tolist()
# heuristically choose columns likely to contain non-English text: name, address, city, state, country, notes
candidates = [c for c in text_cols if any(k in c for k in ["name", "address", "city", "state", "country", "note", "locale"])]
candidates = list(dict.fromkeys(candidates))  # unique preserving order
print("\nText columns detected for possible translation/cleanup:", candidates)

#  Try to import a translator (if available). If not available, we'll flag rows needing translation
translator_available = False
try:
    from langdetect import detect
    from googletrans import Translator
    translator = Translator()
    translator_available = True
    print("\nTranslation libs found: googletrans + langdetect")
except Exception:
    try:
        from langdetect import detect
        translator_available = False
        print("\nlangdetect available but googletrans not available — will mark non-English rows for manual translation.")
    except Exception:
        print("\nNo translation libraries found. Non-ASCII text will be flagged for manual translation.")

def likely_non_english(s):
    if pd.isna(s):
        return False
    s_str = str(s).strip()
    # quick heuristic: presence of characters outside basic ASCII or digits/punctuation
    if any(ord(ch) > 127 for ch in s_str):
        return True
    # short strings often pose detection problems; check for typical english letters ratio
    letters = [ch for ch in s_str if ch.isalpha()]
    if len(letters) == 0:
        return False
    english_letters = sum(1 for ch in letters if ch.lower() in "abcdefghijklmnopqrstuvwxyz")
    # if less than 70% are english alphabet letters, flag
    if english_letters / len(letters) < 0.7:
        return True
    return False

# Apply translation or flagging
translation_report = []
for col in candidates:
    needs = df[col].dropna().apply(lambda x: likely_non_english(x)).sum()
    translation_report.append((col, needs))
print("\nTranslation detection report (counts of rows likely non-English in each candidate column):")
for col, cnt in translation_report:
    print(f"  {col}: {cnt} rows")

# If translator available, attempt to translate small sample and create a translated column
if translator_available:
    for col, _ in translation_report:
        translated_col = f"{col}_translated_en"
        df[translated_col] = df[col].astype(str).fillna("")
        # Only translate rows that look non-english to reduce API calls if library exists
        mask = df[col].fillna("").apply(likely_non_english)
        sample_idxs = df[mask].index.tolist()[:1000]  # safety: translate at most 1000 rows here
        for i in sample_idxs:
            try:
                txt = str(df.at[i, col])
                tr = translator.translate(txt, dest='en')
                df.at[i, translated_col] = tr.text
            except Exception:
                # fallback: leave original
                df.at[i, translated_col] = df.at[i, col]
    print("\nAttempted automated translation — results stored in *_translated_en columns where created.")
else:
    # Create translated columns that simply mirror original but add a flag column for manual translation
    for col, _ in translation_report:
        df[f"{col}_translated_en"] = df[col]
        df[f"{col}_needs_manual_translation"] = df[col].fillna("").apply(likely_non_english)

    print("\nTranslation not automated. *_needs_manual_translation boolean columns created for manual translation guidance.")

#  Geocoding / Coordinates
# Common column names are latitude/longitude variants; try to find them
lat_cols = [c for c in df.columns if "lat" in c]
lon_cols = [c for c in df.columns if "lon" in c or "long" in c or "lng" in c]
print("\nLatitude candidate columns:", lat_cols)
print("Longitude candidate columns:", lon_cols)

if len(lat_cols) == 0 or len(lon_cols) == 0:
    print("No obvious latitude/longitude columns found. Showing columns so you can map them manually.")
    print(df.columns.tolist())
else:
    lat_col = lat_cols[0]
    lon_col = lon_cols[0]
    # convert to numeric and drop rows without coords
    df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')
    df[lon_col] = pd.to_numeric(df[lon_col], errors='coerce')
    before = len(df)
    df = df.dropna(subset=[lat_col, lon_col]).reset_index(drop=True)
    after = len(df)
    print(f"\nDropped {before-after} rows missing coordinates. Remaining rows: {after}")

#  Deduplicate based on coords/address
# Create a simple dedupe key
key_cols = []
for c in ["address", "address_line", "street", "store_number", "id", "station_name"]:
    if c in df.columns:
        key_cols.append(c)
# fallback to lat/lon if no address-like column
if not key_cols:
    key_cols = [lat_col, lon_col]
dedup_before = len(df)
df = df.drop_duplicates(subset=key_cols).reset_index(drop=True)
dedup_after = len(df)
print(f"\nDeduplicated rows using {key_cols}: removed {dedup_before-dedup_after} duplicates. Remaining: {dedup_after}")

#  Basic EDA: store counts by country and city
country_col_candidates = [c for c in df.columns if "country" in c]
country_col = country_col_candidates[0] if country_col_candidates else None
city_col_candidates = [c for c in df.columns if "city" in c]
city_col = city_col_candidates[0] if city_col_candidates else None

if country_col is None:
    print("Country column not found. Country-level aggregation will not run. Columns:")
    print(df.columns.tolist())

# Aggregate
if country_col:
    country_counts = df[country_col].fillna("Unknown").value_counts().reset_index()
    country_counts.columns = ["country", "store_count"]
    top_countries = country_counts.head(20)
    print("\nTop 10 countries by store count:")
    print(top_countries.head(10).to_string(index=False))
    if display_dataframe_to_user:
        display_dataframe_to_user("Top countries by store count", top_countries.head(50))
else:
    country_counts = pd.DataFrame()

if city_col:
    city_counts = df[city_col].fillna("Unknown").value_counts().reset_index()
    city_counts.columns = ["city", "store_count"]
    top_cities = city_counts.head(20)
    print("\nTop 10 cities by store count:")
    print(top_cities.head(10).to_string(index=False))
    if display_dataframe_to_user:
        display_dataframe_to_user("Top cities by store count", top_cities.head(50))
else:
    city_counts = pd.DataFrame()

#  Feature engineering: nearest-neighbor distance (km)
def haversine_array(lat1, lon1, lat2, lon2):
    # all args in radians arrays or scalars
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2
    c = 2 * np.arcsin(np.sqrt(a))
    R = 6371.0  # km
    return R * c

coords = df[[lat_col, lon_col]].to_numpy()
# Convert to radians for haversine metric use in sklearn
coords_rad = np.radians(coords)

# Use NearestNeighbors with haversine metric (k=2 to skip self distance)
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree', metric='haversine').fit(coords_rad)
distances_rad, indices = nbrs.kneighbors(coords_rad)
# distances_rad[:,0] is zero (self); take second column
nn_dist_km = distances_rad[:,1] * 6371.0
df['nn_dist_km'] = nn_dist_km
print("\nNearest-neighbor distance stats (km):")
print(df['nn_dist_km'].describe().to_string())

# Country-level avg nearest neighbor and other features
if country_col:
    country_feats = df.groupby(country_col).agg(
        store_count = (lat_col, 'count'),
        unique_cities = (city_col, lambda x: x.nunique() if city_col else np.nan),
        avg_nn_km = ('nn_dist_km', 'mean'),
        median_nn_km = ('nn_dist_km', 'median')
    ).reset_index().sort_values('store_count', ascending=False)
    country_feats['store_per_city'] = country_feats['store_count'] / country_feats['unique_cities'].replace(0, np.nan)
    print("\nCountry-level features (top 10):")
    print(country_feats.head(10).to_string(index=False))
    if display_dataframe_to_user:
        display_dataframe_to_user("Country-level features", country_feats.head(200))
else:
    country_feats = pd.DataFrame()

#  Clustering stores to find local clusters (DBSCAN with haversine)
# Use DBSCAN with haversine metric: eps in radians (e.g., 1 km = 1/6371 rad)
eps_km = 1.5
eps_rad = eps_km / 6371.0
db = DBSCAN(eps=eps_rad, min_samples=3, metric='haversine').fit(coords_rad)
df['cluster_id'] = db.labels_
n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)
n_noise = list(db.labels_).count(-1)
print(f"\nDBSCAN clustering done. Clusters found (excluding noise): {n_clusters}. Noise points: {n_noise}")

# Summarize clusters: size and centroid
clusters_summary = df[df['cluster_id']!=-1].groupby('cluster_id').agg(
    cluster_size=(lat_col,'count'),
    centroid_lat=(lat_col,'mean'),
    centroid_lon=(lon_col,'mean'),
    avg_nn_km=('nn_dist_km','mean')
).reset_index().sort_values('cluster_size', ascending=False)
print("\nTop cluster examples:")
print(clusters_summary.head(8).to_string(index=False))
if display_dataframe_to_user:
    display_dataframe_to_user("Clusters summary", clusters_summary.head(200))

#  Simple segmentation / opportunity scoring
if not country_feats.empty:
    # compute global stats
    sc_mean = country_feats['store_count'].mean()
    sc_std = country_feats['store_count'].std(ddof=0) if country_feats['store_count'].std(ddof=0) > 0 else 1.0
    # z-score for store_count
    country_feats['store_count_z'] = (country_feats['store_count'] - sc_mean) / sc_std
    # lower avg_nn_km means dense/saturated; higher means sparse/opportunity
    global_median_nn = country_feats['median_nn_km'].median()
    # simple opportunity score: negative store_count_z, plus normalized median_nn_km
    # normalize median_nn_km to 0-1
    mm = country_feats['median_nn_km']
    mm_min, mm_max = mm.min(), mm.max() if mm.max() != mm.min() else (mm.min(), mm.min()+1)
    country_feats['median_nn_norm'] = (mm - mm_min) / (mm_max - mm_min)
    # opportunity_score: higher is more opportunity
    country_feats['opportunity_score'] = (-country_feats['store_count_z']) * 0.6 + country_feats['median_nn_norm'] * 0.4
    # rank
    country_feats = country_feats.sort_values('opportunity_score', ascending=False).reset_index(drop=True)
    country_feats['opportunity_rank'] = country_feats.index + 1
    print("\nTop 10 countries by dataset-only opportunity score:")
    print(country_feats[['country', 'store_count', 'median_nn_km', 'opportunity_score']].head(10).to_string(index=False))
    if display_dataframe_to_user:
        display_dataframe_to_user("Country opportunity ranking", country_feats.head(200))
else:
    print("\nSkipping country-level opportunity ranking (no country column found).")

#  Save cleaned data and outputs
out_dir = "/mnt/data/starbucks_analysis_outputs"
os.makedirs(out_dir, exist_ok=True)
cleaned_csv = os.path.join(out_dir, "starbucks_cleaned.csv")
df.to_csv(cleaned_csv, index=False)
print(f"\nCleaned dataset saved to: {cleaned_csv}")

if not country_feats.empty:
    country_feats.to_csv(os.path.join(out_dir, "country_features_opportunity.csv"), index=False)
    print("Country features and opportunity ranking saved.")

clusters_summary.to_csv(os.path.join(out_dir, "clusters_summary.csv"), index=False)

#  Simple plots
plt.figure(figsize=(8,6))
if country_col and not country_counts.empty:
    topc = country_counts.head(10)
    plt.barh(topc['country'][::-1], topc['store_count'][::-1])
    plt.title("Top 10 countries by Starbucks store count (dataset only)")
    plt.xlabel("Store count")
    plt.tight_layout()
    topc_plot = os.path.join(out_dir, "top10_countries.png")
    plt.savefig(topc_plot)
    print(f"Saved plot: {topc_plot}")
    plt.clf()

# Scatter map of sample points (lat vs lon)
plt.figure(figsize=(9,6))
plt.scatter(df[lon_col].sample(frac=1.0 if len(df)<=5000 else 0.05), df[lat_col].sample(frac=1.0 if len(df)<=5000 else 0.05), s=8, alpha=0.6)
plt.title("Scatter of store locations (sampled if large dataset)")
plt.xlabel("Longitude"); plt.ylabel("Latitude"); plt.tight_layout()
scatter_plot = os.path.join(out_dir, "store_scatter.png")
plt.savefig(scatter_plot)
print(f"Saved plot: {scatter_plot}")
plt.clf()

# Histogram of nearest-neighbor distances
plt.figure(figsize=(8,5))
plt.hist(df['nn_dist_km'], bins=60)
plt.title("Distribution of nearest store distance (km)")
plt.xlabel("Nearest neighbor distance (km)"); plt.ylabel("Count"); plt.tight_layout()
hist_plot = os.path.join(out_dir, "nn_distance_hist.png")
plt.savefig(hist_plot)
print(f"Saved plot: {hist_plot}")
plt.clf()

print("\nAll outputs saved in folder:", out_dir)
print("Files you can use in Power BI / Tableau / PPT:")
for f in os.listdir(out_dir):
    print(" ", os.path.join(out_dir, f))

# Scatter map of store locations using Plotly
import plotly.express as px

lat_col = None
lon_col = None
for c in df.columns:
    if c.lower() in ("latitude", "lat", "lat_deg", "y"):
        lat_col = c
    if c.lower() in ("longitude", "lon", "lng", "long", "x"):
        lon_col = c

if lat_col is None or lon_col is None:
    print("Latitude/Longitude columns not found. Columns:", df.columns.tolist())
else:
    sample_df = df.sample(n=min(len(df), 20000), random_state=1)  # sample for performance
    fig = px.scatter_geo(
        sample_df,
        lat=lat_col,
        lon=lon_col,
        hover_name=sample_df.get("store_name") if "store_name" in sample_df.columns else None,
        hover_data={lat_col: False, lon_col: False, "nn_dist_km": True} if "nn_dist_km" in sample_df.columns else None,
        size_max=6,
        projection="natural earth",
        title="Starbucks store locations (sampled)",
        height=600
    )
    fig.update_traces(marker=dict(size=4, opacity=0.6))
    fig.show()

# Top 10 countries by store_count
if country_feats.empty or 'store_count' not in country_feats.columns:
    print("country_feats missing or no store_count column. Falling back to grouping from df.")
    top_c = df.groupby(df.columns[df.columns.str.contains("country", case=False)][0] if any(df.columns.str.contains("country", case=False)) else df.columns[0]).size().reset_index(name='store_count').sort_values('store_count', ascending=False).head(10)
else:
    top_c = country_feats[['country','store_count']].sort_values('store_count', ascending=False).head(10)

import plotly.express as px
fig = px.bar(top_c.sort_values('store_count'), x='store_count', y=top_c.columns[0], orientation='h', title='Top 10 countries by store count')
fig.update_layout(height=450, margin=dict(l=100))
fig.show()